{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fad09-92a1-4ea2-899e-d346a90456c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal type\n",
      "Load weights b基础633.pth.\n",
      "\n",
      "Successful Load Key: ['backbone.stem.conv.weight', 'backbone.stem.bn.weight', 'backbone.stem.bn.bias', 'backbone.stem.bn.running_mean', 'backbone.stem.bn.running_var', 'backbone.stem.bn.num_batches_tracked', 'backbone.dark2.0.conv.weight', 'backbone.dark2.0.bn.weight', 'backbone.dark2.0.bn.bias', 'backbone.dark2.0.bn.running_mean', 'backbone.dark2.0.bn.running_var', 'backbone.dark2.0.bn.num_batches_tracked', 'backbone.dark2.1.cv1.conv.weight', 'backbone.dark2.1.cv1.bn.weight', 'backbone.dark2.1.cv1.bn.bias', 'backbo ……\n",
      "Successful Load Key Num: 235\n",
      "\n",
      "Fail To Load Key: ['backbone.dark2.1.m.0.cv1.conv.weight', 'backbone.dark2.1.m.0.cv1.bn.weight', 'backbone.dark2.1.m.0.cv1.bn.bias', 'backbone.dark2.1.m.0.cv1.bn.running_mean', 'backbone.dark2.1.m.0.cv1.bn.running_var', 'backbone.dark2.1.m.0.cv1.bn.num_batches_tracked', 'backbone.dark2.1.m.0.cv2.conv.weight', 'backbone.dark2.1.m.0.cv2.bn.weight', 'backbone.dark2.1.m.0.cv2.bn.bias', 'backbone.dark2.1.m.0.cv2.bn.running_mean', 'backbone.dark2.1.m.0.cv2.bn.running_var', 'backbone.dark2.1.m.0.cv2.bn.num_batches_track ……\n",
      "Fail To Load Key num: 120\n",
      "\n",
      "\u001b[1;33;44m温馨提示，head部分没有载入是正常现象，Backbone部分没有载入是错误的。\u001b[0m\n",
      "Configurations:\n",
      "----------------------------------------------------------------------\n",
      "|                     keys |                                   values|\n",
      "----------------------------------------------------------------------\n",
      "|             classes_path |               model_data/voc_classes.txt|\n",
      "|               model_path |                               b基础633.pth|\n",
      "|              input_shape |                               [640, 640]|\n",
      "|               Init_Epoch |                                        0|\n",
      "|             Freeze_Epoch |                                       50|\n",
      "|           UnFreeze_Epoch |                                      300|\n",
      "|        Freeze_batch_size |                                        2|\n",
      "|      Unfreeze_batch_size |                                        4|\n",
      "|             Freeze_Train |                                     True|\n",
      "|                  Init_lr |                                     0.01|\n",
      "|                   Min_lr |                                   0.0001|\n",
      "|           optimizer_type |                                     adam|\n",
      "|                 momentum |                                    0.937|\n",
      "|            lr_decay_type |                                      cos|\n",
      "|              save_period |                                       30|\n",
      "|                 save_dir |                                     logs|\n",
      "|              num_workers |                                        4|\n",
      "|                num_train |                                       90|\n",
      "|                  num_val |                                       10|\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\u001b[1;33;44m[Warning] 使用adam优化器时，建议将训练总步长设置到15000以上。\u001b[0m\n",
      "\u001b[1;33;44m[Warning] 本次运行的总训练数据量为90，Unfreeze_batch_size为4，共训练300个Epoch，计算出总训练步长为6600。\u001b[0m\n",
      "\u001b[1;33;44m[Warning] 由于总训练步长为6600，小于建议总步长15000，建议设置总世代为682。\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████| 45/45 [00:06<00:00,  7.10it/s, loss=12.3, lr=3.13e-5]\n",
      "Epoch 2/300: 100%|██████████| 45/45 [00:04<00:00, 10.12it/s, loss=10.4, lr=6.25e-5]\n",
      "Epoch 3/300: 100%|██████████| 45/45 [00:04<00:00, 10.36it/s, loss=8.85, lr=0.000156]\n",
      "Epoch 4/300: 100%|██████████| 45/45 [00:04<00:00,  9.87it/s, loss=8, lr=0.000313]   \n",
      "Epoch 5/300: 100%|██████████| 45/45 [00:04<00:00, 10.26it/s, loss=7.35, lr=0.000312]\n",
      "Epoch 6/300: 100%|██████████| 45/45 [00:04<00:00, 10.16it/s, loss=7, lr=0.000312]   \n",
      "Epoch 7/300: 100%|██████████| 45/45 [00:04<00:00, 10.31it/s, loss=6.63, lr=0.000312]\n",
      "Epoch 8/300: 100%|██████████| 45/45 [00:04<00:00,  9.93it/s, loss=6.3, lr=0.000312] \n",
      "Epoch 9/300: 100%|██████████| 45/45 [00:04<00:00, 10.62it/s, loss=6.29, lr=0.000312]\n",
      "Epoch 10/300: 100%|██████████| 45/45 [00:04<00:00, 10.70it/s, loss=5.9, lr=0.000312] \n",
      "100%|██████████| 10/10 [00:01<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.84% = 0 AP \t||\tscore_threhold=0.5 : F1=0.06 ; Recall=3.23% ; Precision=100.00%\n",
      "20.25% = 1 AP \t||\tscore_threhold=0.5 : F1=0.00 ; Recall=0.00% ; Precision=0.00%\n",
      "31.54% = 2 AP \t||\tscore_threhold=0.5 : F1=0.00 ; Recall=0.00% ; Precision=0.00%\n",
      "28.02% = 3 AP \t||\tscore_threhold=0.5 : F1=0.00 ; Recall=0.00% ; Precision=0.00%\n",
      "17.06% = 4 AP \t||\tscore_threhold=0.5 : F1=0.00 ; Recall=0.00% ; Precision=0.00%\n",
      "mAP = 20.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/300: 100%|██████████| 45/45 [00:04<00:00,  9.33it/s, loss=5.74, lr=0.000312]\n",
      "Epoch 12/300: 100%|██████████| 45/45 [00:04<00:00, 10.00it/s, loss=5.74, lr=0.000312]\n",
      "Epoch 13/300: 100%|██████████| 45/45 [00:04<00:00,  9.69it/s, loss=5.54, lr=0.000312]\n",
      "Epoch 14/300: 100%|██████████| 45/45 [00:03<00:00, 11.77it/s, loss=5.6, lr=0.000312] \n",
      "Epoch 15/300: 100%|██████████| 45/45 [00:04<00:00, 10.36it/s, loss=5.36, lr=0.000311]\n",
      "Epoch 16/300: 100%|██████████| 45/45 [00:04<00:00,  9.83it/s, loss=5.32, lr=0.000311]\n",
      "Epoch 17/300: 100%|██████████| 45/45 [00:04<00:00, 10.19it/s, loss=5.26, lr=0.000311]\n",
      "Epoch 18/300: 100%|██████████| 45/45 [00:04<00:00, 10.15it/s, loss=5.14, lr=0.000311]\n",
      "Epoch 19/300: 100%|██████████| 45/45 [00:04<00:00,  9.95it/s, loss=5.23, lr=0.00031]\n",
      "Epoch 20/300: 100%|██████████| 45/45 [00:03<00:00, 11.53it/s, loss=4.89, lr=0.00031]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.34% = 0 AP \t||\tscore_threhold=0.5 : F1=0.06 ; Recall=3.23% ; Precision=100.00%\n",
      "43.36% = 1 AP \t||\tscore_threhold=0.5 : F1=0.06 ; Recall=3.17% ; Precision=100.00%\n",
      "56.16% = 2 AP \t||\tscore_threhold=0.5 : F1=0.41 ; Recall=29.03% ; Precision=69.23%\n",
      "46.61% = 3 AP \t||\tscore_threhold=0.5 : F1=0.07 ; Recall=3.70% ; Precision=100.00%\n",
      "11.77% = 4 AP \t||\tscore_threhold=0.5 : F1=0.00 ; Recall=0.00% ; Precision=0.00%\n",
      "mAP = 37.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/300: 100%|██████████| 45/45 [00:04<00:00,  9.80it/s, loss=4.96, lr=0.00031]\n",
      "Epoch 22/300: 100%|██████████| 45/45 [00:04<00:00,  9.96it/s, loss=5.02, lr=0.000309]\n",
      "Epoch 23/300: 100%|██████████| 45/45 [00:04<00:00,  9.65it/s, loss=4.9, lr=0.000309] \n",
      "Epoch 24/300: 100%|██████████| 45/45 [00:04<00:00,  9.87it/s, loss=4.83, lr=0.000309]\n",
      "Epoch 25/300: 100%|██████████| 45/45 [00:04<00:00, 10.21it/s, loss=4.79, lr=0.000308]\n",
      "Epoch 26/300: 100%|██████████| 45/45 [00:04<00:00, 11.21it/s, loss=4.82, lr=0.000308]\n",
      "Epoch 27/300: 100%|██████████| 45/45 [00:04<00:00,  9.42it/s, loss=4.8, lr=0.000307] \n",
      "Epoch 28/300: 100%|██████████| 45/45 [00:04<00:00,  9.45it/s, loss=4.73, lr=0.000307]\n",
      "Epoch 29/300: 100%|██████████| 45/45 [00:04<00:00, 10.37it/s, loss=4.72, lr=0.000307]\n",
      "Epoch 30/300: 100%|██████████| 45/45 [00:04<00:00, 10.85it/s, loss=4.67, lr=0.000306]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.57% = 0 AP \t||\tscore_threhold=0.5 : F1=0.12 ; Recall=6.45% ; Precision=100.00%\n",
      "52.25% = 1 AP \t||\tscore_threhold=0.5 : F1=0.40 ; Recall=30.16% ; Precision=61.29%\n",
      "52.21% = 2 AP \t||\tscore_threhold=0.5 : F1=0.31 ; Recall=19.35% ; Precision=75.00%\n",
      "47.31% = 3 AP \t||\tscore_threhold=0.5 : F1=0.07 ; Recall=3.70% ; Precision=100.00%\n",
      "33.40% = 4 AP \t||\tscore_threhold=0.5 : F1=0.18 ; Recall=14.29% ; Precision=25.00%\n",
      "mAP = 44.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/300: 100%|██████████| 45/45 [00:04<00:00, 10.74it/s, loss=4.65, lr=0.000306]\n",
      "Epoch 32/300: 100%|██████████| 45/45 [00:04<00:00,  9.24it/s, loss=4.6, lr=0.000305] \n",
      "Epoch 33/300: 100%|██████████| 45/45 [00:04<00:00, 10.33it/s, loss=4.58, lr=0.000304]\n",
      "Epoch 34/300: 100%|██████████| 45/45 [00:04<00:00,  9.72it/s, loss=4.53, lr=0.000304]\n",
      "Epoch 35/300: 100%|██████████| 45/45 [00:04<00:00,  9.44it/s, loss=4.61, lr=0.000303]\n",
      "Epoch 36/300: 100%|██████████| 45/45 [00:04<00:00, 10.13it/s, loss=4.37, lr=0.000303]\n",
      "Epoch 37/300: 100%|██████████| 45/45 [00:04<00:00,  9.69it/s, loss=4.45, lr=0.000302]\n",
      "Epoch 38/300: 100%|██████████| 45/45 [00:04<00:00, 10.51it/s, loss=4.6, lr=0.000302] \n",
      "Epoch 39/300: 100%|██████████| 45/45 [00:04<00:00, 10.21it/s, loss=4.41, lr=0.000301]\n",
      "Epoch 40/300: 100%|██████████| 45/45 [00:04<00:00, 10.30it/s, loss=4.47, lr=0.0003]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.52% = 0 AP \t||\tscore_threhold=0.5 : F1=0.12 ; Recall=6.45% ; Precision=66.67%\n",
      "49.20% = 1 AP \t||\tscore_threhold=0.5 : F1=0.24 ; Recall=15.87% ; Precision=52.63%\n",
      "63.40% = 2 AP \t||\tscore_threhold=0.5 : F1=0.58 ; Recall=61.29% ; Precision=55.88%\n",
      "66.23% = 3 AP \t||\tscore_threhold=0.5 : F1=0.07 ; Recall=3.70% ; Precision=100.00%\n",
      "36.79% = 4 AP \t||\tscore_threhold=0.5 : F1=0.36 ; Recall=28.57% ; Precision=50.00%\n",
      "mAP = 49.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/300:  87%|████████▋ | 39/45 [00:04<00:00, 11.18it/s, loss=4.24, lr=0.0003]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from nets.yolo import YoloBody\n",
    "from nets.yolo_training import (Loss, ModelEMA, get_lr_scheduler,\n",
    "                                set_optimizer_lr, weights_init)\n",
    "from utils.callbacks import EvalCallback, LossHistory\n",
    "from utils.dataloader import YoloDataset, yolo_dataset_collate\n",
    "from utils.utils import (download_weights, get_classes, seed_everything,\n",
    "                         show_config, worker_init_fn)\n",
    "from utils.utils_fit import fit_one_epoch\n",
    "if __name__ == \"__main__\":\n",
    "    Cuda            = True\n",
    "    seed            = 11\n",
    "    distributed     = False\n",
    "    sync_bn         = False\n",
    "    fp16            = True\n",
    "    classes_path    = 'model_data/voc_classes.txt'\n",
    "    model_path      = 'b基础633.pth'#'model_data/yolov8_s.pth'\n",
    "    input_shape     = [640, 640]\n",
    "    phi             = 'n'\n",
    "    pretrained      = False\n",
    "    mosaic              = True\n",
    "    mosaic_prob         = 0.5\n",
    "    mixup               = True\n",
    "    mixup_prob          = 0.5\n",
    "    special_aug_ratio   = 0.7\n",
    "    label_smoothing     = 0\n",
    "    Init_Epoch          = 0\n",
    "    Freeze_Epoch        = 50\n",
    "    Freeze_batch_size   = 2\n",
    "    UnFreeze_Epoch      = 300\n",
    "    Unfreeze_batch_size = 4\n",
    "    Freeze_Train        = True\n",
    "    Init_lr             = 1e-2\n",
    "    Min_lr              = Init_lr * 0.01\n",
    "    optimizer_type      = \"adam\"\n",
    "    momentum            = 0.937\n",
    "    weight_decay        = 5e-4\n",
    "    lr_decay_type       = \"cos\"\n",
    "    save_period         = 30\n",
    "    save_dir            = 'logs'\n",
    "    eval_flag           = True\n",
    "    eval_period         = 10\n",
    "    num_workers         = 4\n",
    "    train_annotation_path   = '2007_train.txt'\n",
    "    val_annotation_path     = '2007_val.txt'\n",
    "    seed_everything(seed)\n",
    "    ngpus_per_node  = torch.cuda.device_count()\n",
    "    if distributed:\n",
    "        dist.init_process_group(backend=\"nccl\")\n",
    "        local_rank  = int(os.environ[\"LOCAL_RANK\"])\n",
    "        rank        = int(os.environ[\"RANK\"])\n",
    "        device      = torch.device(\"cuda\", local_rank)\n",
    "        if local_rank == 0:\n",
    "            print(f\"[{os.getpid()}] (rank = {rank}, local_rank = {local_rank}) training...\")\n",
    "            print(\"Gpu Device Count : \", ngpus_per_node)\n",
    "    else:\n",
    "        device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        local_rank      = 0\n",
    "        rank            = 0\n",
    "\n",
    "    #------------------------------------------------------#\n",
    "    #   获取classes和anchor\n",
    "    #------------------------------------------------------#\n",
    "    class_names, num_classes = get_classes(classes_path)\n",
    "\n",
    "    #----------------------------------------------------#\n",
    "    #   下载预训练权重\n",
    "    #----------------------------------------------------#\n",
    "    if pretrained:\n",
    "        if distributed:\n",
    "            if local_rank == 0:\n",
    "                download_weights(phi)  \n",
    "            dist.barrier()\n",
    "        else:\n",
    "            download_weights(phi)\n",
    "            \n",
    "    #------------------------------------------------------#\n",
    "    #   创建yolo模型\n",
    "    #------------------------------------------------------#\n",
    "    model = YoloBody(input_shape, num_classes, phi, pretrained=pretrained)\n",
    "\n",
    "    if model_path != '':\n",
    "        #------------------------------------------------------#\n",
    "        #   权值文件请看README，百度网盘下载\n",
    "        #------------------------------------------------------#\n",
    "        if local_rank == 0:\n",
    "            print('Load weights {}.'.format(model_path))\n",
    "        \n",
    "        #------------------------------------------------------#\n",
    "        #   根据预训练权重的Key和模型的Key进行加载\n",
    "        #------------------------------------------------------#\n",
    "        model_dict      = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_path, map_location = device)\n",
    "        load_key, no_load_key, temp_dict = [], [], {}\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):\n",
    "                temp_dict[k] = v\n",
    "                load_key.append(k)\n",
    "            else:\n",
    "                no_load_key.append(k)\n",
    "        model_dict.update(temp_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        #------------------------------------------------------#\n",
    "        #   显示没有匹配上的Key\n",
    "        #------------------------------------------------------#\n",
    "        if local_rank == 0:\n",
    "            print(\"\\nSuccessful Load Key:\", str(load_key)[:500], \"……\\nSuccessful Load Key Num:\", len(load_key))\n",
    "            print(\"\\nFail To Load Key:\", str(no_load_key)[:500], \"……\\nFail To Load Key num:\", len(no_load_key))\n",
    "            print(\"\\n\\033[1;33;44m温馨提示，head部分没有载入是正常现象，Backbone部分没有载入是错误的。\\033[0m\")\n",
    "\n",
    "    #----------------------#\n",
    "    #   获得损失函数\n",
    "    #----------------------#\n",
    "    yolo_loss = Loss(model)\n",
    "    #----------------------#\n",
    "    #   记录Loss\n",
    "    #----------------------#\n",
    "    if local_rank == 0:\n",
    "        time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n",
    "        log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n",
    "        loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n",
    "    else:\n",
    "        loss_history    = None\n",
    "        \n",
    "    #------------------------------------------------------------------#\n",
    "    #   torch 1.2不支持amp，建议使用torch 1.7.1及以上正确使用fp16\n",
    "    #   因此torch1.2这里显示\"could not be resolve\"\n",
    "    #------------------------------------------------------------------#\n",
    "    if fp16:\n",
    "        from torch.cuda.amp import GradScaler as GradScaler\n",
    "        scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    model_train     = model.train()\n",
    "    #----------------------------#\n",
    "    #   多卡同步Bn\n",
    "    #----------------------------#\n",
    "    if sync_bn and ngpus_per_node > 1 and distributed:\n",
    "        model_train = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_train)\n",
    "    elif sync_bn:\n",
    "        print(\"Sync_bn is not support in one gpu or not distributed.\")\n",
    "\n",
    "    if Cuda:\n",
    "        if distributed:\n",
    "            #----------------------------#\n",
    "            #   多卡平行运行\n",
    "            #----------------------------#\n",
    "            model_train = model_train.cuda(local_rank)\n",
    "            model_train = torch.nn.parallel.DistributedDataParallel(model_train, device_ids=[local_rank], find_unused_parameters=True)\n",
    "        else:\n",
    "            model_train = torch.nn.DataParallel(model)\n",
    "            cudnn.benchmark = True\n",
    "            model_train = model_train.cuda()\n",
    "            \n",
    "    #----------------------------#\n",
    "    #   权值平滑\n",
    "    #----------------------------#\n",
    "    ema = ModelEMA(model_train)\n",
    "    \n",
    "    #---------------------------#\n",
    "    #   读取数据集对应的txt\n",
    "    #---------------------------#\n",
    "    with open(train_annotation_path, encoding='utf-8') as f:\n",
    "        train_lines = f.readlines()\n",
    "    with open(val_annotation_path, encoding='utf-8') as f:\n",
    "        val_lines   = f.readlines()\n",
    "    num_train   = len(train_lines)\n",
    "    num_val     = len(val_lines)\n",
    "\n",
    "    if local_rank == 0:\n",
    "        show_config(\n",
    "            classes_path = classes_path, model_path = model_path, input_shape = input_shape, \\\n",
    "            Init_Epoch = Init_Epoch, Freeze_Epoch = Freeze_Epoch, UnFreeze_Epoch = UnFreeze_Epoch, Freeze_batch_size = Freeze_batch_size, Unfreeze_batch_size = Unfreeze_batch_size, Freeze_Train = Freeze_Train, \\\n",
    "            Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n",
    "            save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n",
    "        )\n",
    "        #---------------------------------------------------------#\n",
    "        #   总训练世代指的是遍历全部数据的总次数\n",
    "        #   总训练步长指的是梯度下降的总次数 \n",
    "        #   每个训练世代包含若干训练步长，每个训练步长进行一次梯度下降。\n",
    "        #   此处仅建议最低训练世代，上不封顶，计算时只考虑了解冻部分\n",
    "        #----------------------------------------------------------#\n",
    "        wanted_step = 5e4 if optimizer_type == \"sgd\" else 1.5e4\n",
    "        total_step  = num_train // Unfreeze_batch_size * UnFreeze_Epoch\n",
    "        if total_step <= wanted_step:\n",
    "            if num_train // Unfreeze_batch_size == 0:\n",
    "                raise ValueError('数据集过小，无法进行训练，请扩充数据集。')\n",
    "            wanted_epoch = wanted_step // (num_train // Unfreeze_batch_size) + 1\n",
    "            print(\"\\n\\033[1;33;44m[Warning] 使用%s优化器时，建议将训练总步长设置到%d以上。\\033[0m\"%(optimizer_type, wanted_step))\n",
    "            print(\"\\033[1;33;44m[Warning] 本次运行的总训练数据量为%d，Unfreeze_batch_size为%d，共训练%d个Epoch，计算出总训练步长为%d。\\033[0m\"%(num_train, Unfreeze_batch_size, UnFreeze_Epoch, total_step))\n",
    "            print(\"\\033[1;33;44m[Warning] 由于总训练步长为%d，小于建议总步长%d，建议设置总世代为%d。\\033[0m\"%(total_step, wanted_step, wanted_epoch))\n",
    "\n",
    "    #------------------------------------------------------#\n",
    "    #   主干特征提取网络特征通用，冻结训练可以加快训练速度\n",
    "    #   也可以在训练初期防止权值被破坏。\n",
    "    #   Init_Epoch为起始世代\n",
    "    #   Freeze_Epoch为冻结训练的世代\n",
    "    #   UnFreeze_Epoch总训练世代\n",
    "    #   提示OOM或者显存不足请调小Batch_size\n",
    "    #------------------------------------------------------#\n",
    "    if True:\n",
    "        UnFreeze_flag = False\n",
    "        #------------------------------------#\n",
    "        #   冻结一定部分训练\n",
    "        #------------------------------------#\n",
    "        if Freeze_Train:\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        #-------------------------------------------------------------------#\n",
    "        #   如果不冻结训练的话，直接设置batch_size为Unfreeze_batch_size\n",
    "        #-------------------------------------------------------------------#\n",
    "        batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n",
    "\n",
    "        #-------------------------------------------------------------------#\n",
    "        #   判断当前batch_size，自适应调整学习率\n",
    "        #-------------------------------------------------------------------#\n",
    "        nbs             = 64\n",
    "        lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
    "        lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
    "        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "\n",
    "        #---------------------------------------#\n",
    "        #   根据optimizer_type选择优化器\n",
    "        #---------------------------------------#\n",
    "        pg0, pg1, pg2 = [], [], []  \n",
    "        for k, v in model.named_modules():\n",
    "            if hasattr(v, \"bias\") and isinstance(v.bias, nn.Parameter):\n",
    "                pg2.append(v.bias)    \n",
    "            if isinstance(v, nn.BatchNorm2d) or \"bn\" in k:\n",
    "                pg0.append(v.weight)    \n",
    "            elif hasattr(v, \"weight\") and isinstance(v.weight, nn.Parameter):\n",
    "                pg1.append(v.weight)   \n",
    "        optimizer = {\n",
    "            'adam'  : optim.Adam(pg0, Init_lr_fit, betas = (momentum, 0.999)),\n",
    "            'sgd'   : optim.SGD(pg0, Init_lr_fit, momentum = momentum, nesterov=True)\n",
    "        }[optimizer_type]\n",
    "        optimizer.add_param_group({\"params\": pg1, \"weight_decay\": weight_decay})\n",
    "        optimizer.add_param_group({\"params\": pg2})\n",
    "\n",
    "        #---------------------------------------#\n",
    "        #   获得学习率下降的公式\n",
    "        #---------------------------------------#\n",
    "        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "        \n",
    "        #---------------------------------------#\n",
    "        #   判断每一个世代的长度\n",
    "        #---------------------------------------#\n",
    "        epoch_step      = num_train // batch_size\n",
    "        epoch_step_val  = num_val // batch_size\n",
    "        \n",
    "        if epoch_step == 0 or epoch_step_val == 0:\n",
    "            raise ValueError(\"数据集过小，无法继续进行训练，请扩充数据集。\")\n",
    "\n",
    "        if ema:\n",
    "            ema.updates     = epoch_step * Init_Epoch\n",
    "        \n",
    "        #---------------------------------------#\n",
    "        #   构建数据集加载器。\n",
    "        #---------------------------------------#\n",
    "        train_dataset   = YoloDataset(train_lines, input_shape, num_classes, epoch_length=UnFreeze_Epoch, \\\n",
    "                                        mosaic=mosaic, mixup=mixup, mosaic_prob=mosaic_prob, mixup_prob=mixup_prob, train=True, special_aug_ratio=special_aug_ratio)\n",
    "        val_dataset     = YoloDataset(val_lines, input_shape, num_classes, epoch_length=UnFreeze_Epoch, \\\n",
    "                                        mosaic=False, mixup=False, mosaic_prob=0, mixup_prob=0, train=False, special_aug_ratio=0)\n",
    "        \n",
    "        if distributed:\n",
    "            train_sampler   = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True,)\n",
    "            val_sampler     = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False,)\n",
    "            batch_size      = batch_size // ngpus_per_node\n",
    "            shuffle         = False\n",
    "        else:\n",
    "            train_sampler   = None\n",
    "            val_sampler     = None\n",
    "            shuffle         = True\n",
    "\n",
    "        gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
    "                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler, \n",
    "                                    worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "        gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n",
    "                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler, \n",
    "                                    worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "\n",
    "        #----------------------#\n",
    "        #   记录eval的map曲线\n",
    "        #----------------------#\n",
    "        if local_rank == 0:\n",
    "            eval_callback   = EvalCallback(model, input_shape, class_names, num_classes, val_lines, log_dir, Cuda, \\\n",
    "                                            eval_flag=eval_flag, period=eval_period)\n",
    "        else:\n",
    "            eval_callback   = None\n",
    "        \n",
    "        #---------------------------------------#\n",
    "        #   开始模型训练\n",
    "        #---------------------------------------#\n",
    "        for epoch in range(Init_Epoch, UnFreeze_Epoch):\n",
    "            #---------------------------------------#\n",
    "            #   如果模型有冻结学习部分\n",
    "            #   则解冻，并设置参数\n",
    "            #---------------------------------------#\n",
    "            if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train:\n",
    "                batch_size = Unfreeze_batch_size\n",
    "\n",
    "                #-------------------------------------------------------------------#\n",
    "                #   判断当前batch_size，自适应调整学习率\n",
    "                #-------------------------------------------------------------------#\n",
    "                nbs             = 64\n",
    "                lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n",
    "                lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n",
    "                Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n",
    "                Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n",
    "                #---------------------------------------#\n",
    "                #   获得学习率下降的公式\n",
    "                #---------------------------------------#\n",
    "                lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n",
    "\n",
    "                for param in model.backbone.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "                epoch_step      = num_train // batch_size\n",
    "                epoch_step_val  = num_val // batch_size\n",
    "\n",
    "                if epoch_step == 0 or epoch_step_val == 0:\n",
    "                    raise ValueError(\"数据集过小，无法继续进行训练，请扩充数据集。\")\n",
    "                    \n",
    "                if ema:\n",
    "                    ema.updates     = epoch_step * epoch\n",
    "\n",
    "                if distributed:\n",
    "                    batch_size  = batch_size // ngpus_per_node\n",
    "                    \n",
    "                gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n",
    "                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler, \n",
    "                                            worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "                gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n",
    "                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler, \n",
    "                                            worker_init_fn=partial(worker_init_fn, rank=rank, seed=seed))\n",
    "\n",
    "                UnFreeze_flag   = True\n",
    "\n",
    "            gen.dataset.epoch_now       = epoch\n",
    "            gen_val.dataset.epoch_now   = epoch\n",
    "\n",
    "            if distributed:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "\n",
    "            set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n",
    "\n",
    "            fit_one_epoch(model_train, model, ema, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, fp16, scaler, save_period, save_dir, local_rank)\n",
    "            \n",
    "            if distributed:\n",
    "                dist.barrier()\n",
    "\n",
    "        if local_rank == 0:\n",
    "            loss_history.writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
